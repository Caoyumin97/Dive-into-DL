# Task 04- æœºå™¨ç¿»è¯‘-æ³¨æ„åŠ›æœºåˆ¶-Seq2seq-Transformer

## 1 æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼‰

### 1.0 æ•°æ®é›†å½¢å¼

![image-20200218130134681](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\1.png)

### 1.1 åºåˆ—é¢„å¤„ç†

* åºåˆ—paddingï¼ˆä¿æŒåºåˆ—çš„é•¿åº¦ä¸€è‡´ï¼‰--> valid lengthï¼šåºåˆ—æœªpaddingä¹‹å‰çš„é•¿åº¦

```python
def pad(line, max_len, padding_token):
    if len(line) > max_len:
        return line[:max_len]
    return line + [padding_token] * (max_len - len(line))
```

### 1.2 Seq2Seqæ¨¡å‹ï¼ˆEncoder-Decoderæ¡†æ¶ï¼‰

* Encoderï¼šè¾“å…¥åºåˆ—ç¼–ç åˆ°è¯­ä¹‰ç¼–ç ï¼ˆcontext vector $\bold{c}$ï¼‰
* Decoderï¼šå°†è¯­ä¹‰ç¼–ç è§£ç åˆ°è¾“å‡º

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\2.png)

* $Seq2Seq$æ¨¡å‹æ¶æ„

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\3.png)

* è®­ç»ƒï¼š

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\4.png)

* æµ‹è¯•ï¼šå¤šæ¬¡å‰å‘ä¼ æ’­ï¼Œè¿›è¡Œè‡ªå›å½’é¢„æµ‹

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\5.png)

### 1.3 æ¨¡å‹æµ‹è¯•åºåˆ—ç”Ÿæˆç­–ç•¥

* è´ªå¿ƒæœç´¢ï¼ˆgreedy searchï¼‰ï¼šé€æ­¥é€‰å–æ¦‚ç‡æœ€å¤§çš„è¯

![image-20200218131658061](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\6.png)

* ç©·ä¸¾æœç´¢ï¼ˆexhaustive searchï¼‰ï¼šç©·ä¸¾æ‰€æœ‰å¯èƒ½è¾“å‡ºåºåˆ—ï¼Œä½†å¼€é”€è¿‡å¤§

* é›†æŸæœç´¢ï¼ˆbeam searchï¼‰ï¼šç»´ç‰¹æ¯”ç®—æ³•ï¼Œå¸¦æœ‰è¶…å‚æ•°æŸå®½ï¼ˆbeam sizeï¼‰$k$ï¼Œåœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥é€‰å–$k$ä¸ªæ¦‚ç‡æœ€å¤§çš„è¯ï¼Œåœ¨å…¶åçš„æ—¶é—´æ­¥å„é€‰å–ä¸€ä¸ªæ¦‚ç‡æœ€å¤§çš„è¯

![image-20200218131639886](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\7.png)

## 2 æ³¨æ„åŠ›æœºåˆ¶

### 2.0 ä¸ºä»€ä¹ˆï¼Ÿ

* Enc-Decæ¡†æ¶é‡Œï¼ŒDecoderå°†è¯­ä¹‰ç¼–ç æ˜ å°„è‡³è¾“å‡ºï¼›
* $Seq2Seq$æ¨¡å‹å› ä¸ºä½¿ç”¨RNNè¿›è¡Œç¼–/è§£ç ï¼Œå®¹æ˜“å‡ºç°é•¿ç¨‹æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œæ•…è¯­ä¹‰ç¼–ç éš¾ä»¥ä¿å­˜å®Œæ•´è¯­ä¹‰ä¿¡æ¯ï¼›
* å„ä¸ªè¾“å‡ºè¯å¯¹å„ä¸ªè¾“å…¥è¯çš„ä¾èµ–ç¨‹åº¦ä¸åŒï¼Œè¿™ç§ä¾èµ–ç¨‹åº¦é€šè¿‡æƒé‡è¡¨è¾¾å³$attention\ weights$

### 2.1 ç»„æˆ

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\8.png)

* Queryï¼ˆ$ğª$ï¼‰ï¼šç†è§£ä¸ºç”±è¾“å‡ºç«¯å‘è¾“å…¥ç«¯å‘å‡ºä¸€ä¸ªæŸ¥è¯¢è¯·æ±‚ï¼Œé€šè¿‡$attention\ weights$è¿›è¡Œç¼–ç 
* Keyï¼ˆ$ğ¤_ğ‘–$ï¼‰-Valueï¼ˆ$ğ¯_ğ‘–$ï¼‰ï¼šé”®-å€¼å¯¹

$$
ğ¤_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘˜}, ğ¯_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘£}, ğªâˆˆâ„^{ğ‘‘_ğ‘}.
\\
a_i = \alpha(\mathbf q, \mathbf k_i).
\\
b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).
\\
\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.
$$

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\9.png)

### 2.2 åˆ†ç±»

#### 2.2.1 ç‚¹ç§¯æ³¨æ„åŠ›

$$
\forall i, ğª,ğ¤_ğ‘– âˆˆ â„_ğ‘‘
\\
ğ›¼(ğª,ğ¤)=âŸ¨ğª,ğ¤âŸ©/ \sqrt{d}
\\
ğ›¼(ğ,ğŠ)=ğğŠ^ğ‘‡/\sqrt{d}\ (ğâˆˆâ„^{ğ‘šÃ—ğ‘‘},ğŠâˆˆâ„^{ğ‘›Ã—ğ‘‘})
$$

#### 2.2.2 MLPæ³¨æ„åŠ›

$$
ğ–_ğ‘˜âˆˆâ„^{â„Ã—ğ‘‘_ğ‘˜},ğ–_ğ‘âˆˆâ„^{â„Ã—ğ‘‘_ğ‘},ğ¯âˆˆâ„^h
\\
ğ›¼(ğ¤,ğª)=ğ¯^ğ‘‡tanh(ğ–_ğ‘˜ğ¤+ğ–_ğ‘ğª)
$$

### 2.3 Enc-Decä¸Šçš„æ³¨æ„åŠ›æœºåˆ¶

![image-20200218133715021](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\10.png)



## 3 Transformer

![image-20200218133756947](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\11.png)

### 3.0 Self-Attention

* $Q,K,V$å‡ä¸ºå½“å‰æ—¶é—´æ­¥çš„å€¼ï¼Œç¬¬ä¸€æ¬¡softmaxåï¼Œä¸å‰åæ—¶é—´æ­¥å†è¿›è¡Œä¸€æ¬¡attentionè®¡ç®—

![Fig. 10.3.2 è‡ªæ³¨æ„åŠ›ç»“æ„](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\12.png)

```python
class DotProductAttention(nn.Module): 
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # query: (batch_size, #queries, d)
    # key: (batch_size, #kv_pairs, d)
    # value: (batch_size, #kv_pairs, dim_v)
    # valid_length: either (batch_size, ) or (batch_size, xx)
    def forward(self, query, key, value, valid_length=None):
        d = query.shape[-1]
        # set transpose_b=True to swap the last two dimensions of key
        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)
        attention_weights = self.dropout(masked_softmax(scores, valid_length))
        return torch.bmm(attention_weights, value)
```



### 3.1 Multi-head Attention

* å¤šå¤´æ³¨æ„åŠ›ï¼šå°†queryã€keyå’Œvalueç”¨ä¸‰ä¸ªç°è¡Œå±‚è¿›è¡Œæ˜ å°„ï¼Œè¿™hä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºå°†ä¼šè¢«æ‹¼æ¥ä¹‹åè¾“å…¥æœ€åä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œæ•´åˆ

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\13.png)

```python
def transpose_qkv(X, num_heads):
    # Original X shape: (batch_size, seq_len, hidden_size * num_heads),
    # -1 means inferring its value, after first reshape, X shape:
    # (batch_size, seq_len, num_heads, hidden_size)
    X = X.view(X.shape[0], X.shape[1], num_heads, -1)
    
    # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)
    X = X.transpose(2, 1).contiguous()

    # Merge the first two dimensions. Use reverse=True to infer shape from
    # right to left.
    # output shape: (batch_size * num_heads, seq_len, hidden_size)
    output = X.view(-1, X.shape[2], X.shape[3])
    return output


# Saved in the d2l package for later use
def transpose_output(X, num_heads):
    # A reversed version of transpose_qkv
    X = X.view(-1, num_heads, X.shape[1], X.shape[2])
    X = X.transpose(2, 1).contiguous()
    return X.view(X.shape[0], X.shape[1], -1)



class MultiHeadAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = DotProductAttention(dropout)
        self.W_q = nn.Linear(input_size, hidden_size, bias=False)
        self.W_k = nn.Linear(input_size, hidden_size, bias=False)
        self.W_v = nn.Linear(input_size, hidden_size, bias=False)
        self.W_o = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, query, key, value, valid_length):
        # query, key, and value shape: (batch_size, seq_len, dim),
        # where seq_len is the length of input sequence
        # valid_length shape is either (batch_size, )
        # or (batch_size, seq_len).

        # Project and transpose query, key, and value from
        # (batch_size, seq_len, hidden_size * num_heads) to
        # (batch_size * num_heads, seq_len, hidden_size).
        
        query = transpose_qkv(self.W_q(query), self.num_heads)
        key = transpose_qkv(self.W_k(key), self.num_heads)
        value = transpose_qkv(self.W_v(value), self.num_heads)
        
        if valid_length is not None:
            # Copy valid_length by num_heads times
            device = valid_length.device
            valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy()
            if valid_length.ndim == 1:
                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))
            else:
                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1)))

            valid_length = valid_length.to(device)
            
        output = self.attention(query, key, value, valid_length)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
```



### 3.2 Positional Encoding

* ç”¨æ³¢å‡½æ•°æ¥å¯¹åºåˆ—ä½ç½®è¿›è¡Œç¼–ç 

$$
P_{i,2j} = sin(i/10000^{2j/d})
\\
P_{i,2j+1} = cos(i/10000^{2j/d})
\\
for\ i=0,\ldots, l-1\ and\ j=0,\ldots,\lfloor (d-1)/2 \rfloor
$$

![Image Name](C:\Users\DELL\Desktop\Dive-into-DL\Task 04\14.png)

### 3.3 Position-wise FeedForward Network (FFN)

* ä»…å¯¹è¾“å…¥çš„æœ€åä¸€ç»´è¿›è¡Œç¼–ç ï¼Œå³æ¯ä¸ªè¯çš„embeddingè¿›è¡Œç¼–ç 

```python
class PositionWiseFFN(nn.Module):
    def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)
        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)
        
        
    def forward(self, X):
        return self.ffn_2(F.relu(self.ffn_1(X)))
```

### 3.4 Transformer code

```python
class EncoderBlock(nn.Module):
    def __init__(self, embedding_size, ffn_hidden_size, num_heads,
                 dropout, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
        self.addnorm_1 = AddNorm(embedding_size, dropout)
        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)
        self.addnorm_2 = AddNorm(embedding_size, dropout)

    def forward(self, X, valid_length):
        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))
        return self.addnorm_2(Y, self.ffn(Y))
        
        
class TransformerEncoder(d2l.Encoder):
    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.embedding_size = embedding_size
        self.embed = nn.Embedding(vocab_size, embedding_size)
        self.pos_encoding = PositionalEncoding(embedding_size, dropout)
        self.blks = nn.ModuleList()
        for i in range(num_layers):
            self.blks.append(
                EncoderBlock(embedding_size, ffn_hidden_size,
                             num_heads, dropout))

    def forward(self, X, valid_length, *args):
        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))
        for blk in self.blks:
            X = blk(X, valid_length)
        return X


class DecoderBlock(nn.Module):
    def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
        self.addnorm_1 = AddNorm(embedding_size, dropout)
        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
        self.addnorm_2 = AddNorm(embedding_size, dropout)
        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)
        self.addnorm_3 = AddNorm(embedding_size, dropout)
    
    def forward(self, X, state):
        enc_outputs, enc_valid_length = state[0], state[1]
        
        # state[2][self.i] stores all the previous t-1 query state of layer-i
        # len(state[2]) = num_layers
        
        if state[2][self.i] is None:
            key_values = X
        else:
            # shape of key_values = (batch_size, t, hidden_size)
            key_values = torch.cat((state[2][self.i], X), dim=1) 
        state[2][self.i] = key_values
        
        if self.training:
            batch_size, seq_len, _ = X.shape
            # Shape: (batch_size, seq_len), the values in the j-th column are j+1
            valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) 
            valid_length = valid_length.to(X.device)
        else:
            valid_length = None

        X2 = self.attention_1(X, key_values, key_values, valid_length)
        Y = self.addnorm_1(X, X2)
        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)
        Z = self.addnorm_2(Y, Y2)
        return self.addnorm_3(Z, self.ffn(Z)), state
    
    

class TransformerDecoder(d2l.Decoder):
    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.embedding_size = embedding_size
        self.num_layers = num_layers
        self.embed = nn.Embedding(vocab_size, embedding_size)
        self.pos_encoding = PositionalEncoding(embedding_size, dropout)
        self.blks = nn.ModuleList()
        for i in range(num_layers):
            self.blks.append(
                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,
                             dropout, i))
        self.dense = nn.Linear(embedding_size, vocab_size)

    def init_state(self, enc_outputs, enc_valid_length, *args):
        return [enc_outputs, enc_valid_length, [None]*self.num_layers]

    def forward(self, X, state):
        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))
        for blk in self.blks:
            X, state = blk(X, state)
        return self.dense(X), state
```

